{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style type='text/css'>\n",
       ".datatable table.frame { margin-bottom: 0; }\n",
       ".datatable table.frame thead { border-bottom: none; }\n",
       ".datatable table.frame tr.coltypes td {  color: #FFFFFF;  line-height: 6px;  padding: 0 0.5em;}\n",
       ".datatable .bool    { background: #DDDD99; }\n",
       ".datatable .object  { background: #565656; }\n",
       ".datatable .int     { background: #5D9E5D; }\n",
       ".datatable .float   { background: #4040CC; }\n",
       ".datatable .str     { background: #CC4040; }\n",
       ".datatable .time    { background: #40CC40; }\n",
       ".datatable .row_index {  background: var(--jp-border-color3);  border-right: 1px solid var(--jp-border-color0);  color: var(--jp-ui-font-color3);  font-size: 9px;}\n",
       ".datatable .frame tbody td { text-align: left; }\n",
       ".datatable .frame tr.coltypes .row_index {  background: var(--jp-border-color0);}\n",
       ".datatable th:nth-child(2) { padding-left: 12px; }\n",
       ".datatable .hellipsis {  color: var(--jp-cell-editor-border-color);}\n",
       ".datatable .vellipsis {  background: var(--jp-layout-color0);  color: var(--jp-cell-editor-border-color);}\n",
       ".datatable .na {  color: var(--jp-cell-editor-border-color);  font-size: 80%;}\n",
       ".datatable .sp {  opacity: 0.25;}\n",
       ".datatable .footer { font-size: 9px; }\n",
       ".datatable .frame_dimensions {  background: var(--jp-border-color3);  border-top: 1px solid var(--jp-border-color0);  color: var(--jp-ui-font-color3);  display: inline-block;  opacity: 0.6;  padding: 1px 10px 1px 5px;}\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torchvision import transforms\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.utils import resample\n",
    "from sklearn.metrics import f1_score, recall_score, accuracy_score, confusion_matrix, roc_auc_score, balanced_accuracy_score\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "import datatable as dt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import glob\n",
    "import UQ_toolbox as uq\n",
    "from PIL import Image\n",
    "from skimage import io\n",
    "%matplotlib widget\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of GPUs available: 3\n",
      "GPU 0: NVIDIA RTX A6000\n",
      "GPU 1: NVIDIA RTX A6000\n",
      "GPU 2: NVIDIA RTX A6000\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    num_gpus = torch.cuda.device_count()\n",
    "    print(\"Number of GPUs available:\", num_gpus)\n",
    "    for i in range(num_gpus):\n",
    "        print(f\"GPU {i}: {torch.cuda.get_device_name(i)}\")\n",
    "except Exception as e:\n",
    "    print(\"Error while checking GPUs:\", e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "print(torch.cuda.is_available())\n",
    "device = torch.device(\"cuda:1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define your CNN model\n",
    "class simpleNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(simpleNet, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 16, kernel_size=3, stride=1, padding=1)\n",
    "        self.bn1 = nn.BatchNorm2d(16)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.pool = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        self.dropout_conv = nn.Dropout(0.3)\n",
    "        self.conv2 = nn.Conv2d(16, 32, kernel_size=3, stride=1, padding=1)\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.fc1 = nn.Linear(32*12*12, 64)\n",
    "        self.dropout = nn.Dropout(p=0.5)\n",
    "        self.fc2 = nn.Linear(64, 1)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.pool(self.relu(self.conv1(x)))\n",
    "        x= self.dropout_conv(x)\n",
    "        x = self.pool(self.relu(self.conv2(x)))\n",
    "        x = self.dropout_conv(x)\n",
    "        x = self.flatten(x)\n",
    "        x = self.relu(self.fc1(x))\n",
    "        x = self.dropout(x)\n",
    "        x = self.fc2(x)\n",
    "        x = self.sigmoid(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ToTensor(object):\n",
    "\n",
    "    def __call__(self, sample):\n",
    "        image, shape, img_name = sample['image'], sample['shape'], sample['name']\n",
    "        if shape=='Round':\n",
    "            shape=0\n",
    "        elif shape=='Irregular':\n",
    "            shape=1\n",
    "        elif shape=='Ambiguous':\n",
    "            shape=2\n",
    "        return {'image': torch.from_numpy(image).unsqueeze(0),\n",
    "                'shape': torch.from_numpy(np.asarray(shape)),\n",
    "                'name': img_name}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Normalize(object):\n",
    "    def __init__(self, mean, std):\n",
    "        self.mean = mean\n",
    "        self.std = std\n",
    "    def __call__(self, sample):\n",
    "        image, shape, img_name = sample['image'], sample['shape'], sample['name']\n",
    "        norm = transforms.Normalize(mean=self.mean, std=self.std)\n",
    "        return {'image': norm(image.float()),\n",
    "                'shape': shape,            \n",
    "                'name': img_name}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the models\n",
    "model_paths = ['model_shape_0_augmented.pt', 'model_shape_1_augmented.pt', 'model_shape_2_augmented.pt', 'model_shape_3_augmented.pt', 'model_shape_4_augmented.pt']\n",
    "models_list = []\n",
    "path_to_models = './models/'\n",
    "for path in model_paths:\n",
    "    model = simpleNet()\n",
    "    model.load_state_dict(torch.load(path_to_models + path))\n",
    "    model.eval()  # Set the model to evaluation mode\n",
    "    models_list.append(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AxialCutsDataset(Dataset):\n",
    "\n",
    "    def __init__(self, data_shape, transform=None, for_trainning=False, mean=False, std=False, downsample=False):\n",
    "        self.data = data_shape\n",
    "        self.transform=transform\n",
    "        self.for_trainning = for_trainning\n",
    "        self.mean= mean\n",
    "        self.std = std\n",
    "        self.downsample = downsample\n",
    "\n",
    "        if self.downsample:\n",
    "            df_majority = self.data[self.data.iloc[:, 1]=='Irregular']\n",
    "            df_majority_downsampled = resample(df_majority, replace=False, n_samples=1200, random_state=125)\n",
    "            self.data = pd.concat((self.data[self.data['Shape'] != 'Irregular'], df_majority_downsampled))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data.iloc[:, 0])\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        if torch.is_tensor(idx):\n",
    "            idx = idx.tolist()\n",
    "        img_name = self.data.iloc[idx, 0]\n",
    "        image = io.imread(img_name)\n",
    "        shape = self.data.iloc[idx, 1]\n",
    "        sample = {'image': image, 'shape': shape, 'name':img_name}\n",
    "\n",
    "        if self.transform:\n",
    "            sample = self.transform(sample)\n",
    "\n",
    "        return sample\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define functions for evaluation metrics\n",
    "def accuracy(outputs, labels):\n",
    "    preds = outputs > 0.5 \n",
    "    return accuracy_score(labels, preds)\n",
    "\n",
    "def f1(outputs, labels):\n",
    "    preds = outputs > 0.5\n",
    "    return f1_score(labels, preds, average='binary')\n",
    "\n",
    "def calculate_sensitivity(outputs, labels):\n",
    "    preds = outputs > 0.5\n",
    "    return recall_score(labels, preds, average='binary')\n",
    "\n",
    "def roc_auc(outputs, labels):\n",
    "    probs = outputs\n",
    "    return roc_auc_score(labels, probs)\n",
    "\n",
    "def compute_confusion_matrix(outputs, labels):\n",
    "    preds = outputs > 0.5\n",
    "    return confusion_matrix(labels, preds)\n",
    "\n",
    "def calculate_specificity(cm):\n",
    "    TN = cm[0, 0]  # True negatives\n",
    "    FP = cm[0, 1]  # False positives\n",
    "    return TN / (TN + FP)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_confusion_matrix(cm):\n",
    "    # Define class names\n",
    "    class_names = ['Round', 'Irregular']\n",
    "    \n",
    "    # Create a DataFrame for better visualization\n",
    "    df_cm = pd.DataFrame(cm, index=class_names, columns=class_names)\n",
    "    \n",
    "    # Plot the confusion matrix\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    sns.heatmap(df_cm, annot=True, fmt='d', cmap='Blues', cbar=False, annot_kws={\"size\": 20})\n",
    "    plt.xlabel('Predicted')\n",
    "    plt.ylabel('Actual')\n",
    "    plt.title('Confusion Matrix')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_val_metrics(all_preds, all_labels):\n",
    "\n",
    "    # Compute evaluation metrics\n",
    "    acc = accuracy(all_preds, all_labels)\n",
    "    f1_result = f1(all_preds, all_labels)\n",
    "    roc_auc_result = roc_auc(all_preds, all_labels)\n",
    "    cm = compute_confusion_matrix(all_preds, all_labels)\n",
    "    display_confusion_matrix(cm)\n",
    "    sensitivity_value = calculate_sensitivity(all_preds, all_labels)\n",
    "    specificity_value = calculate_specificity(cm)\n",
    "\n",
    "    print('Accuracy: {:.6f} \\tF1 Score: {:.6f} \\tROC AUC: {:.6f} \\tSpecificity: {:.6f} \\tSensitivity: {:.6f}'.format(\n",
    "        acc, f1_result, roc_auc_result, specificity_value, sensitivity_value))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to make predictions\n",
    "def predict(models, image):\n",
    "    image = image.to(device)\n",
    "    predictions = [model(image).cpu().detach().numpy() for model in models]\n",
    "    return predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_tumor_predictions_with_percentages(data, shape_df):\n",
    "\n",
    "    # Process and aggregate data for the bar plot\n",
    "    tumor_data = {}\n",
    "    \n",
    "    for entry in data:\n",
    "        img_name = entry['img_name']\n",
    "        predicted_class = entry['predicted_class']\n",
    "        # Extract patient (tumor) number and cut number from the image name\n",
    "        tumor_id, cut_id_with_ext = img_name.split('/')[-1].split('_')[:2]\n",
    "        cut_id = cut_id_with_ext.split('.')[0]  # Remove .png extension\n",
    "        \n",
    "        if tumor_id not in tumor_data:\n",
    "            tumor_data[tumor_id] = []\n",
    "        \n",
    "        # Add predicted class (0 for round, 1 for irregular) to tumor's list\n",
    "        tumor_data[tumor_id].append((int(cut_id), predicted_class))\n",
    "    \n",
    "    # Sort cuts by their axial number within each tumor\n",
    "    for tumor_id in tumor_data:\n",
    "        tumor_data[tumor_id].sort(key=lambda x: x[0])\n",
    "    \n",
    "    # Create bar plot\n",
    "    fig, ax = plt.subplots(figsize=(8, 10))\n",
    "    \n",
    "    # For each tumor, create a row of colored bars based on the axial cut predictions\n",
    "    y_positions = np.arange(len(tumor_data))\n",
    "    \n",
    "    for i, (tumor_id, cuts) in enumerate(sorted(tumor_data.items())):\n",
    "     \n",
    "        # Extract cut numbers and predicted classes\n",
    "        cut_ids, predicted_classes = zip(*cuts)\n",
    "        \n",
    "        # Create a color map: red for 'Round' (class 0) and blue for 'Irregular' (class 1)\n",
    "        colors = ['red' if pred == 0 else 'blue' for pred in predicted_classes]\n",
    "        \n",
    "        # Initialize variables to keep track of blocks of the same color\n",
    "        current_color = colors[0]\n",
    "        block_start = cut_ids[0]\n",
    "        block_count = 1\n",
    "        \n",
    "        total_cuts = len(cuts)\n",
    "        \n",
    "        for j in range(1, total_cuts):\n",
    "            if colors[j] == current_color:\n",
    "                # Continue the current block\n",
    "                block_count += 1\n",
    "            else:\n",
    "                # End the current block, plot it\n",
    "                ax.barh(i, block_count, left=block_start, height=0.8, color=current_color)\n",
    "                \n",
    "                # Calculate the percentage for the block\n",
    "                percentage = (block_count / total_cuts) * 100\n",
    "                \n",
    "                # Add the percentage as text in the middle of the block\n",
    "                ax.text(block_start + block_count / 2, i, f'{percentage:.1f}%', va='center', ha='center', color='white')\n",
    "                \n",
    "                # Start a new block\n",
    "                current_color = colors[j]\n",
    "                block_start = cut_ids[j]\n",
    "                block_count = 1\n",
    "        \n",
    "        # Plot the final block\n",
    "        ax.barh(i, block_count, left=block_start, height=0.8, color=current_color)\n",
    "        percentage = (block_count / total_cuts) * 100\n",
    "        ax.text(block_start + block_count / 2, i, f'{percentage:.1f}%', va='center', ha='center', color='white')\n",
    "        \n",
    "        # Get the shape from the dataframe based on the tumor ID\n",
    "        try:\n",
    "            shape_label = shape_df.loc[shape_df['Record_ID'] == int(tumor_id), 'Shape.1'].values[0]\n",
    "        except IndexError:\n",
    "            shape_label = 'Unknown'  # Assign a default value if not found\n",
    "\n",
    "        # Add the shape label as text at the end of the bar\n",
    "        ax.text(max(cut_ids) + 1, i, shape_label, va='center')\n",
    "    \n",
    "    ax.set_xlabel('Axial Cuts')\n",
    "    ax.set_ylabel('Tumors')\n",
    "    ax.set_title('Tumor Axial Cut Predictions with Percentages and Shape Labels')\n",
    "    ax.set_yticks(y_positions)\n",
    "    ax.set_yticklabels(sorted(tumor_data.keys()))\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def classify_tumors_by_contingent_blocks_with_sorting(data, shape_df, threshold=60):\n",
    "    y_true = []\n",
    "    y_pred = []\n",
    "    \n",
    "    # Process and aggregate data for each tumor\n",
    "    tumor_data = {}\n",
    "    \n",
    "    for entry in data:\n",
    "        img_name = entry['img_name']\n",
    "        predicted_class = entry['predicted_class']\n",
    "        \n",
    "        # Extract patient (tumor) number and cut number from the image name\n",
    "        tumor_id, cut_id_with_ext = img_name.split('/')[-1].split('_')[:2]\n",
    "        cut_id = int(cut_id_with_ext.split('.')[0])  # Remove .png extension and convert to integer for sorting\n",
    "        \n",
    "        if tumor_id not in tumor_data:\n",
    "            tumor_data[tumor_id] = []\n",
    "        \n",
    "        # Append both cut_id and predicted class to ensure sorting\n",
    "        tumor_data[tumor_id].append((cut_id, predicted_class))\n",
    "    \n",
    "    # Classify each tumor based on the largest contiguous block of \"Round\" cuts\n",
    "    for tumor_id, cuts in tumor_data.items():\n",
    "        # Sort cuts by cut_id to ensure the correct sequence\n",
    "        cuts_sorted = sorted(cuts, key=lambda x: x[0])  # Sort by cut_id\n",
    "        \n",
    "        # Extract only the predicted classes from sorted data\n",
    "        predicted_classes_sorted = [pred_class for _, pred_class in cuts_sorted]\n",
    "        \n",
    "        #print(f\"Processing tumor ID: {tumor_id}\")\n",
    "        \n",
    "        # Find the true label from the shape_df dataframe ('Shape.1' column)\n",
    "        try:\n",
    "            shape_label = shape_df.loc[shape_df['Record_ID'] == int(tumor_id), 'Shape.1'].values[0]\n",
    "            true_label = 0 if shape_label == 'Round' else 1  # Convert 'Round' to 0, 'Irregular' to 1\n",
    "            #print(f\"True label (from Shape.1): {shape_label}, Converted to: {true_label}\")\n",
    "        except IndexError:\n",
    "            #print(f\"Tumor ID {tumor_id} not found in shape_df!\")\n",
    "            continue  # Skip if tumor_id not found in shape_df\n",
    "        \n",
    "        # Calculate the largest contiguous block of \"Round\" (class 0) cuts\n",
    "        max_round_block = 0\n",
    "        current_round_block = 0\n",
    "        total_cuts = len(predicted_classes_sorted)\n",
    "        \n",
    "        for pred_class in predicted_classes_sorted:\n",
    "            if pred_class == 0:\n",
    "                current_round_block += 1\n",
    "            else:\n",
    "                current_round_block = 0\n",
    "            max_round_block = max(max_round_block, current_round_block)\n",
    "        \n",
    "        round_percentage = (max_round_block / total_cuts) * 100\n",
    "        #print(f\"Largest contiguous round block: {max_round_block} out of {total_cuts} cuts ({round_percentage:.1f}%)\")\n",
    "        \n",
    "        # Apply the classification rule: Round if any block is >= threshold, else Irregular\n",
    "        predicted_label = 0 if round_percentage >= threshold else 1\n",
    "        #print(f\"Predicted label based on threshold: {predicted_label}\")\n",
    "        \n",
    "        # Store true and predicted labels\n",
    "        y_true.append(true_label)\n",
    "        y_pred.append(predicted_label)\n",
    "    \n",
    "    # Compute confusion matrix\n",
    "    cm = confusion_matrix(y_true, y_pred)\n",
    "    \n",
    "    # Compute balanced accuracy\n",
    "    bal_acc = balanced_accuracy_score(y_true, y_pred)\n",
    "    \n",
    "    # Compute sensitivity (recall for class 1, irregular)\n",
    "    sensitivity = recall_score(y_true, y_pred, pos_label=1)\n",
    "    \n",
    "    # Compute specificity (recall for class 0, round)\n",
    "    specificity = recall_score(y_true, y_pred, pos_label=0)\n",
    "    \n",
    "    return cm, bal_acc, sensitivity, specificity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "mean = 87.42158495776914\n",
    "std = 29.82248099334633"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean = 87.42158495776914\n",
    "std = 29.82248099334633"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "images_path = '/mnt/data/psteinmetz/neotex/data_CNN/model_evaluation/evaluation_IRM_villes/'\n",
    "data = pd.concat(\n",
    "    (\n",
    "        pd.DataFrame(glob.glob(f'{images_path}*/*png')),\n",
    "        pd.DataFrame(\n",
    "            [x.split('/')[-1][:-4] for x in glob.glob(f'{images_path}*/*png')]\n",
    "        ),\n",
    "        pd.DataFrame(\n",
    "            [k.split('/')[-2] for k in glob.glob(f'{images_path}*/*png')]\n",
    "        ),\n",
    "    ),\n",
    "    axis=1,\n",
    ")\n",
    "data.columns = ['Path', 'ID', 'Shape']\n",
    "data.set_index('ID', inplace=True)\n",
    "\n",
    "\n",
    "axialcuts_dataset_eval = AxialCutsDataset(data_shape=data, downsample=False)\n",
    "data_without_amb = axialcuts_dataset_eval.data[axialcuts_dataset_eval.data['Shape']!='Ambiguous']\n",
    "data_amb = axialcuts_dataset_eval.data[axialcuts_dataset_eval.data['Shape']=='Ambiguous']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_transforms = transforms.Compose([\n",
    "    ToTensor(),\n",
    "    Normalize(mean=mean, std=std)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_without_amb = AxialCutsDataset(data_shape=data_without_amb, downsample=False, transform=data_transforms)\n",
    "data_amb = AxialCutsDataset(data_shape=data_amb, downsample=False, transform=data_transforms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_data = DataLoader(data_without_amb, num_workers=12)\n",
    "eval_data_amb = DataLoader(data_amb, num_workers=12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "axialcuts_dataset_eval_all = AxialCutsDataset(data_shape=data, downsample=False, transform=data_transforms)\n",
    "eval_all_data = DataLoader(axialcuts_dataset_eval_all, num_workers=12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "axialcuts_dataset_eval_all_gps = AxialCutsDataset(data_shape=data, downsample=False, transform=False)\n",
    "eval_all_data_gps = DataLoader(axialcuts_dataset_eval_all_gps, num_workers=12, batch_size=128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "models = [model.to(device) for model in models_list]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "augmentation n:0\n",
      "Resampled transform. Current transform: \n",
      "[(12,36.81959711641001),(1,31.54362199406961)]\n",
      "Resampled transform: [('Brightness', 36.81959711641001), ('ShearX', 31.54362199406961)]\n",
      "augmentation n:1\n",
      "Resampled transform. Current transform: \n",
      "[(7,23.95484212946009),(2,-6.193800030202716)]\n",
      "Resampled transform: [('Solarize', 23.95484212946009), ('ShearY', -6.193800030202716)]\n",
      "augmentation n:2\n",
      "Resampled transform. Current transform: \n",
      "[(10,-36.587382470185275),(0,9.598417171188252)]\n",
      "Resampled transform: [('Contrast', -36.587382470185275), ('Identity', 9.598417171188252)]\n",
      "augmentation n:3\n",
      "Resampled transform. Current transform: \n",
      "[(12,-4.246642361956454),(0,41.62190361366251)]\n",
      "Resampled transform: [('Brightness', -4.246642361956454), ('Identity', 41.62190361366251)]\n",
      "augmentation n:4\n",
      "Resampled transform. Current transform: \n",
      "[(5,-34.58644526665664),(2,-10.795304923132697)]\n",
      "Resampled transform: [('Rotate', -34.58644526665664), ('ShearY', -10.795304923132697)]\n",
      "augmentation n:5\n",
      "Resampled transform. Current transform: \n",
      "[(10,-12.115922125650016),(7,-5.366693677851622)]\n",
      "Resampled transform: [('Contrast', -12.115922125650016), ('Solarize', -5.366693677851622)]\n",
      "augmentation n:6\n",
      "Resampled transform. Current transform: \n",
      "[(5,-25.88968029605112),(8,-20.31658710152176)]\n",
      "Resampled transform: [('Rotate', -25.88968029605112), ('SolarizeAdd', -20.31658710152176)]\n",
      "augmentation n:7\n",
      "Resampled transform. Current transform: \n",
      "[(7,-9.669289471734018),(5,-29.370971780683497)]\n",
      "Resampled transform: [('Solarize', -9.669289471734018), ('Rotate', -29.370971780683497)]\n",
      "augmentation n:8\n",
      "Resampled transform. Current transform: \n",
      "[(8,-8.409814909968716),(10,38.01787817743495)]\n",
      "Resampled transform: [('SolarizeAdd', -8.409814909968716), ('Contrast', 38.01787817743495)]\n",
      "augmentation n:9\n",
      "Resampled transform. Current transform: \n",
      "[(13,-6.5137220785420595),(0,-16.104799610729913)]\n",
      "Resampled transform: [('Sharpness', -6.5137220785420595), ('Identity', -16.104799610729913)]\n",
      "augmentation n:10\n",
      "Resampled transform. Current transform: \n",
      "[(8,2.835886046670744),(3,21.49862012245616)]\n",
      "Resampled transform: [('SolarizeAdd', 2.835886046670744), ('TranslateX', 21.49862012245616)]\n",
      "augmentation n:11\n",
      "Resampled transform. Current transform: \n",
      "[(5,-35.146607544138035),(14,6.741753888858085)]\n",
      "Resampled transform: [('Rotate', -35.146607544138035), ('Cutout', 6.741753888858085)]\n",
      "augmentation n:12\n",
      "Resampled transform. Current transform: \n",
      "[(8,42.33074836391857),(7,43.640095657528605)]\n",
      "Resampled transform: [('SolarizeAdd', 42.33074836391857), ('Solarize', 43.640095657528605)]\n",
      "augmentation n:13\n",
      "Resampled transform. Current transform: \n",
      "[(11,2.2008074766978325),(4,-22.78018640658816)]\n",
      "Resampled transform: [('Color', 2.2008074766978325), ('TranslateY', -22.78018640658816)]\n",
      "augmentation n:14\n",
      "Resampled transform. Current transform: \n",
      "[(3,3.5720161704842113),(6,-4.425945899483423)]\n",
      "Resampled transform: [('TranslateX', 3.5720161704842113), ('AutoContrast', -4.425945899483423)]\n",
      "augmentation n:15\n",
      "Resampled transform. Current transform: \n",
      "[(1,-24.668038440377835),(4,-14.061264691394882)]\n",
      "Resampled transform: [('ShearX', -24.668038440377835), ('TranslateY', -14.061264691394882)]\n",
      "augmentation n:16\n",
      "Resampled transform. Current transform: \n",
      "[(13,20.938277258026318),(11,-0.705341477938056)]\n",
      "Resampled transform: [('Sharpness', 20.938277258026318), ('Color', -0.705341477938056)]\n",
      "augmentation n:17\n",
      "Resampled transform. Current transform: \n",
      "[(4,4.925748481788638),(11,-19.234875161476584)]\n",
      "Resampled transform: [('TranslateY', 4.925748481788638), ('Color', -19.234875161476584)]\n",
      "augmentation n:18\n",
      "Resampled transform. Current transform: \n",
      "[(1,-31.44108713469486),(1,-19.440800539864966)]\n",
      "Resampled transform: [('ShearX', -31.44108713469486), ('ShearX', -19.440800539864966)]\n",
      "augmentation n:19\n",
      "Resampled transform. Current transform: \n",
      "[(1,-43.42268277502239),(2,-24.21291968819755)]\n",
      "Resampled transform: [('ShearX', -43.42268277502239), ('ShearY', -24.21291968819755)]\n",
      "augmentation n:20\n",
      "Resampled transform. Current transform: \n",
      "[(4,-17.510457105747548),(13,14.023630536433856)]\n",
      "Resampled transform: [('TranslateY', -17.510457105747548), ('Sharpness', 14.023630536433856)]\n",
      "augmentation n:21\n",
      "Resampled transform. Current transform: \n",
      "[(1,-24.287448831053428),(10,-15.160534106769287)]\n",
      "Resampled transform: [('ShearX', -24.287448831053428), ('Contrast', -15.160534106769287)]\n",
      "augmentation n:22\n",
      "Resampled transform. Current transform: \n",
      "[(2,42.364861370887056),(9,29.396161887489882)]\n",
      "Resampled transform: [('ShearY', 42.364861370887056), ('Posterize', 29.396161887489882)]\n",
      "augmentation n:23\n",
      "Resampled transform. Current transform: \n",
      "[(0,-11.275432219442258),(12,-44.83695501491834)]\n",
      "Resampled transform: [('Identity', -11.275432219442258), ('Brightness', -44.83695501491834)]\n",
      "augmentation n:24\n",
      "Resampled transform. Current transform: \n",
      "[(3,13.348526324406556),(2,42.7970305893366)]\n",
      "Resampled transform: [('TranslateX', 13.348526324406556), ('ShearY', 42.7970305893366)]\n",
      "augmentation n:25\n",
      "Resampled transform. Current transform: \n",
      "[(14,2.4664489342000238),(8,43.8962860998828)]\n",
      "Resampled transform: [('Cutout', 2.4664489342000238), ('SolarizeAdd', 43.8962860998828)]\n",
      "augmentation n:26\n",
      "Resampled transform. Current transform: \n",
      "[(5,27.168242521707825),(7,-42.146230942985035)]\n",
      "Resampled transform: [('Rotate', 27.168242521707825), ('Solarize', -42.146230942985035)]\n",
      "augmentation n:27\n",
      "Resampled transform. Current transform: \n",
      "[(0,-9.443537571095789),(3,35.62107525896661)]\n",
      "Resampled transform: [('Identity', -9.443537571095789), ('TranslateX', 35.62107525896661)]\n",
      "augmentation n:28\n",
      "Resampled transform. Current transform: \n",
      "[(14,-24.40455486821254),(2,-4.829986771498163)]\n",
      "Resampled transform: [('Cutout', -24.40455486821254), ('ShearY', -4.829986771498163)]\n",
      "augmentation n:29\n",
      "Resampled transform. Current transform: \n",
      "[(13,-7.3911825552334705),(0,31.24732372254678)]\n",
      "Resampled transform: [('Sharpness', -7.3911825552334705), ('Identity', 31.24732372254678)]\n",
      "augmentation n:30\n",
      "Resampled transform. Current transform: \n",
      "[(9,-28.940686469579326),(9,-29.28870458897366)]\n",
      "Resampled transform: [('Posterize', -28.940686469579326), ('Posterize', -29.28870458897366)]\n",
      "augmentation n:31\n",
      "Resampled transform. Current transform: \n",
      "[(8,10.071367149770893),(10,17.759597582700508)]\n",
      "Resampled transform: [('SolarizeAdd', 10.071367149770893), ('Contrast', 17.759597582700508)]\n",
      "augmentation n:32\n",
      "Resampled transform. Current transform: \n",
      "[(9,18.721134253220747),(10,41.022639882653195)]\n",
      "Resampled transform: [('Posterize', 18.721134253220747), ('Contrast', 41.022639882653195)]\n",
      "augmentation n:33\n",
      "Resampled transform. Current transform: \n",
      "[(3,10.535710745107203),(11,27.17469611152289)]\n",
      "Resampled transform: [('TranslateX', 10.535710745107203), ('Color', 27.17469611152289)]\n",
      "augmentation n:34\n",
      "Resampled transform. Current transform: \n",
      "[(5,12.48904646674275),(0,26.607447722788095)]\n",
      "Resampled transform: [('Rotate', 12.48904646674275), ('Identity', 26.607447722788095)]\n",
      "augmentation n:35\n",
      "Resampled transform. Current transform: \n",
      "[(8,-31.550203696018876),(5,36.317411584311586)]\n",
      "Resampled transform: [('SolarizeAdd', -31.550203696018876), ('Rotate', 36.317411584311586)]\n",
      "augmentation n:36\n",
      "Resampled transform. Current transform: \n",
      "[(12,29.483439594988027),(8,-17.70679375951808)]\n",
      "Resampled transform: [('Brightness', 29.483439594988027), ('SolarizeAdd', -17.70679375951808)]\n",
      "augmentation n:37\n",
      "Resampled transform. Current transform: \n",
      "[(8,12.16607093962223),(6,-34.470063114737535)]\n",
      "Resampled transform: [('SolarizeAdd', 12.16607093962223), ('AutoContrast', -34.470063114737535)]\n",
      "augmentation n:38\n",
      "Resampled transform. Current transform: \n",
      "[(8,-22.76327019763739),(5,-31.498218761123802)]\n",
      "Resampled transform: [('SolarizeAdd', -22.76327019763739), ('Rotate', -31.498218761123802)]\n",
      "augmentation n:39\n",
      "Resampled transform. Current transform: \n",
      "[(14,15.04033510283265),(3,-26.877896550217844)]\n",
      "Resampled transform: [('Cutout', 15.04033510283265), ('TranslateX', -26.877896550217844)]\n",
      "augmentation n:40\n",
      "Resampled transform. Current transform: \n",
      "[(4,-11.789346176144342),(8,-21.624148376510643)]\n",
      "Resampled transform: [('TranslateY', -11.789346176144342), ('SolarizeAdd', -21.624148376510643)]\n",
      "augmentation n:41\n",
      "Resampled transform. Current transform: \n",
      "[(8,-40.0260384142966),(2,30.92402089460502)]\n",
      "Resampled transform: [('SolarizeAdd', -40.0260384142966), ('ShearY', 30.92402089460502)]\n",
      "augmentation n:42\n",
      "Resampled transform. Current transform: \n",
      "[(13,37.39549617542622),(10,2.4407142881645143)]\n",
      "Resampled transform: [('Sharpness', 37.39549617542622), ('Contrast', 2.4407142881645143)]\n",
      "augmentation n:43\n",
      "Resampled transform. Current transform: \n",
      "[(11,41.12675187133165),(1,-10.695091209184426)]\n",
      "Resampled transform: [('Color', 41.12675187133165), ('ShearX', -10.695091209184426)]\n",
      "augmentation n:44\n",
      "Resampled transform. Current transform: \n",
      "[(8,9.838786014824208),(13,-19.37626488322747)]\n",
      "Resampled transform: [('SolarizeAdd', 9.838786014824208), ('Sharpness', -19.37626488322747)]\n",
      "augmentation n:45\n",
      "Resampled transform. Current transform: \n",
      "[(5,35.153703346778826),(13,-24.818950010521217)]\n",
      "Resampled transform: [('Rotate', 35.153703346778826), ('Sharpness', -24.818950010521217)]\n",
      "augmentation n:46\n",
      "Resampled transform. Current transform: \n",
      "[(5,44.538951732189645),(3,-0.9264935694626004)]\n",
      "Resampled transform: [('Rotate', 44.538951732189645), ('TranslateX', -0.9264935694626004)]\n",
      "augmentation n:47\n",
      "Resampled transform. Current transform: \n",
      "[(7,-34.682232082145475),(12,16.917661398160043)]\n",
      "Resampled transform: [('Solarize', -34.682232082145475), ('Brightness', 16.917661398160043)]\n",
      "augmentation n:48\n",
      "Resampled transform. Current transform: \n",
      "[(12,29.831073940320337),(0,29.910430205783726)]\n",
      "Resampled transform: [('Brightness', 29.831073940320337), ('Identity', 29.910430205783726)]\n",
      "augmentation n:49\n",
      "Resampled transform. Current transform: \n",
      "[(12,-8.473112710566411),(10,44.175363527648784)]\n",
      "Resampled transform: [('Brightness', -8.473112710566411), ('Contrast', 44.175363527648784)]\n",
      "augmentation n:50\n",
      "Resampled transform. Current transform: \n",
      "[(12,28.10872487399547),(13,-13.697093001640564)]\n",
      "Resampled transform: [('Brightness', 28.10872487399547), ('Sharpness', -13.697093001640564)]\n",
      "augmentation n:51\n",
      "Resampled transform. Current transform: \n",
      "[(0,2.8893069655177754),(13,34.20281968039039)]\n",
      "Resampled transform: [('Identity', 2.8893069655177754), ('Sharpness', 34.20281968039039)]\n",
      "augmentation n:52\n",
      "Resampled transform. Current transform: \n",
      "[(11,37.687802505624106),(3,0.857316931734502)]\n",
      "Resampled transform: [('Color', 37.687802505624106), ('TranslateX', 0.857316931734502)]\n",
      "augmentation n:53\n",
      "Resampled transform. Current transform: \n",
      "[(8,12.083424975073875),(4,-32.43697913988424)]\n",
      "Resampled transform: [('SolarizeAdd', 12.083424975073875), ('TranslateY', -32.43697913988424)]\n",
      "augmentation n:54\n",
      "Resampled transform. Current transform: \n",
      "[(13,17.28313125620643),(14,-25.350283133484922)]\n",
      "Resampled transform: [('Sharpness', 17.28313125620643), ('Cutout', -25.350283133484922)]\n",
      "augmentation n:55\n",
      "Resampled transform. Current transform: \n",
      "[(13,-7.695445317527096),(5,-39.74998216649961)]\n",
      "Resampled transform: [('Sharpness', -7.695445317527096), ('Rotate', -39.74998216649961)]\n",
      "augmentation n:56\n",
      "Resampled transform. Current transform: \n",
      "[(13,12.494095910478215),(13,8.207028986744113)]\n",
      "Resampled transform: [('Sharpness', 12.494095910478215), ('Sharpness', 8.207028986744113)]\n",
      "augmentation n:57\n",
      "Resampled transform. Current transform: \n",
      "[(2,-24.386153760640028),(0,-29.12542709799599)]\n",
      "Resampled transform: [('ShearY', -24.386153760640028), ('Identity', -29.12542709799599)]\n",
      "augmentation n:58\n",
      "Resampled transform. Current transform: \n",
      "[(1,-33.27661885673638),(0,-0.2016276653131044)]\n",
      "Resampled transform: [('ShearX', -33.27661885673638), ('Identity', -0.2016276653131044)]\n",
      "augmentation n:59\n",
      "Resampled transform. Current transform: \n",
      "[(4,-10.880922591390835),(6,1.7515510585631944)]\n",
      "Resampled transform: [('TranslateY', -10.880922591390835), ('AutoContrast', 1.7515510585631944)]\n",
      "augmentation n:60\n",
      "Resampled transform. Current transform: \n",
      "[(6,2.1427453055900756),(8,4.206330600672963)]\n",
      "Resampled transform: [('AutoContrast', 2.1427453055900756), ('SolarizeAdd', 4.206330600672963)]\n",
      "augmentation n:61\n",
      "Resampled transform. Current transform: \n",
      "[(12,19.248875640501367),(8,37.55454852913209)]\n",
      "Resampled transform: [('Brightness', 19.248875640501367), ('SolarizeAdd', 37.55454852913209)]\n",
      "augmentation n:62\n",
      "Resampled transform. Current transform: \n",
      "[(6,-12.226123474332681),(6,-2.1478954877307217)]\n",
      "Resampled transform: [('AutoContrast', -12.226123474332681), ('AutoContrast', -2.1478954877307217)]\n",
      "augmentation n:63\n",
      "Resampled transform. Current transform: \n",
      "[(10,-9.44168927644084),(6,12.465885940900748)]\n",
      "Resampled transform: [('Contrast', -9.44168927644084), ('AutoContrast', 12.465885940900748)]\n",
      "augmentation n:64\n",
      "Resampled transform. Current transform: \n",
      "[(1,1.710399344120063),(12,20.487775021765728)]\n",
      "Resampled transform: [('ShearX', 1.710399344120063), ('Brightness', 20.487775021765728)]\n",
      "augmentation n:65\n",
      "Resampled transform. Current transform: \n",
      "[(11,-22.258887600109496),(9,12.906112015382469)]\n",
      "Resampled transform: [('Color', -22.258887600109496), ('Posterize', 12.906112015382469)]\n",
      "augmentation n:66\n",
      "Resampled transform. Current transform: \n",
      "[(9,-42.27360636532129),(8,-33.004416682695975)]\n",
      "Resampled transform: [('Posterize', -42.27360636532129), ('SolarizeAdd', -33.004416682695975)]\n",
      "augmentation n:67\n",
      "Resampled transform. Current transform: \n",
      "[(7,40.747277401704636),(13,34.19571759520079)]\n",
      "Resampled transform: [('Solarize', 40.747277401704636), ('Sharpness', 34.19571759520079)]\n",
      "augmentation n:68\n",
      "Resampled transform. Current transform: \n",
      "[(9,28.166971226622323),(13,-1.1402326571221408)]\n",
      "Resampled transform: [('Posterize', 28.166971226622323), ('Sharpness', -1.1402326571221408)]\n",
      "augmentation n:69\n",
      "Resampled transform. Current transform: \n",
      "[(11,-38.249997335307015),(2,44.79698043048752)]\n",
      "Resampled transform: [('Color', -38.249997335307015), ('ShearY', 44.79698043048752)]\n",
      "augmentation n:70\n",
      "Resampled transform. Current transform: \n",
      "[(4,27.73406824530035),(1,1.522347423194681)]\n",
      "Resampled transform: [('TranslateY', 27.73406824530035), ('ShearX', 1.522347423194681)]\n",
      "augmentation n:71\n",
      "Resampled transform. Current transform: \n",
      "[(3,2.9259020961508355),(3,-12.780203686240988)]\n",
      "Resampled transform: [('TranslateX', 2.9259020961508355), ('TranslateX', -12.780203686240988)]\n",
      "augmentation n:72\n",
      "Resampled transform. Current transform: \n",
      "[(9,-13.973096647448997),(1,-36.95077155088881)]\n",
      "Resampled transform: [('Posterize', -13.973096647448997), ('ShearX', -36.95077155088881)]\n",
      "augmentation n:73\n",
      "Resampled transform. Current transform: \n",
      "[(2,-23.71836830002933),(13,16.08779563728443)]\n",
      "Resampled transform: [('ShearY', -23.71836830002933), ('Sharpness', 16.08779563728443)]\n",
      "augmentation n:74\n",
      "Resampled transform. Current transform: \n",
      "[(7,-11.46698557188629),(2,-13.933525702935825)]\n",
      "Resampled transform: [('Solarize', -11.46698557188629), ('ShearY', -13.933525702935825)]\n",
      "augmentation n:75\n",
      "Resampled transform. Current transform: \n",
      "[(1,-29.955675482668827),(10,-23.157327424192076)]\n",
      "Resampled transform: [('ShearX', -29.955675482668827), ('Contrast', -23.157327424192076)]\n",
      "augmentation n:76\n",
      "Resampled transform. Current transform: \n",
      "[(12,-35.31532633138128),(14,-26.271472949263757)]\n",
      "Resampled transform: [('Brightness', -35.31532633138128), ('Cutout', -26.271472949263757)]\n",
      "augmentation n:77\n",
      "Resampled transform. Current transform: \n",
      "[(1,5.331126922002262),(3,-16.50979403081414)]\n",
      "Resampled transform: [('ShearX', 5.331126922002262), ('TranslateX', -16.50979403081414)]\n",
      "augmentation n:78\n",
      "Resampled transform. Current transform: \n",
      "[(5,7.248769573695753),(14,32.110822788119265)]\n",
      "Resampled transform: [('Rotate', 7.248769573695753), ('Cutout', 32.110822788119265)]\n",
      "augmentation n:79\n",
      "Resampled transform. Current transform: \n",
      "[(2,-42.34982975002899),(2,-10.13496912162666)]\n",
      "Resampled transform: [('ShearY', -42.34982975002899), ('ShearY', -10.13496912162666)]\n",
      "augmentation n:80\n",
      "Resampled transform. Current transform: \n",
      "[(1,-26.212055505231927),(8,-8.837684392557883)]\n",
      "Resampled transform: [('ShearX', -26.212055505231927), ('SolarizeAdd', -8.837684392557883)]\n",
      "augmentation n:81\n",
      "Resampled transform. Current transform: \n",
      "[(7,-12.001403449658383),(6,26.975285855501113)]\n",
      "Resampled transform: [('Solarize', -12.001403449658383), ('AutoContrast', 26.975285855501113)]\n",
      "augmentation n:82\n",
      "Resampled transform. Current transform: \n",
      "[(13,-10.817015324028638),(10,-1.9742304401887267)]\n",
      "Resampled transform: [('Sharpness', -10.817015324028638), ('Contrast', -1.9742304401887267)]\n"
     ]
    }
   ],
   "source": [
    "results, name = uq.apply_randaugment_and_store_results(eval_all_data_gps, models, 2, 45, 500, device, binary_classification=True, batch_norm=False, mean=mean, std=std)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Store results\n",
    "all_results = []\n",
    "mean_pred = []\n",
    "true_labels = []\n",
    "models = [model.to(device) for model in models_list]\n",
    "# Inference\n",
    "with torch.no_grad():\n",
    "    for batch in eval_data:\n",
    "        images = batch['image']\n",
    "        labels = batch['shape']\n",
    "\n",
    "        pred_probs = predict(models, images)\n",
    "        \n",
    "        # Collect the results\n",
    "        mean_pred.append(np.mean(pred_probs))\n",
    "        true_labels.append(labels.item())\n",
    "        all_results.append({\n",
    "            'true_label': labels.item(),\n",
    "            'predicted_probabilities': pred_probs,\n",
    "            'predicted_class': int(np.mean(pred_probs) > 0.5),\n",
    "            'std': np.std(pred_probs),\n",
    "            'mean': np.mean(pred_probs)\n",
    "        })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "models_pred = [[all_results[k]['predicted_probabilities'][i].ravel() for k in range(len(all_results))] for i in range(5)]\n",
    "mean_ensembling_pred = [all_results[k]['mean'] for k in range(len(all_results))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "calculate_val_metrics(np.array(mean_pred), np.array(true_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "uq.model_calibration_plot(np.array(true_labels), np.array(mean_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "good_idx = [k for k in range(len(all_results)) if all_results[k]['true_label'] == all_results[k]['predicted_class']]\n",
    "bad_idx = [k for k in range(len(all_results)) if all_results[k]['true_label'] != all_results[k]['predicted_class']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "stds = uq.ensembling_stds_computation(models_pred)\n",
    "distances = uq.distance_to_gold_standard_computation(mean_ensembling_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "uq.UQ_method_plot([stds[k] for k in good_idx], [stds[j] for j in bad_idx], 'Stds', 'Standard deviations ensembling')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scatter_with_density():\n",
    "    plt.close('all')\n",
    "    # The 5 horizontal points to be scattered\n",
    "    points = np.array([0.2, 0.45, 0.33, 0.38, 0.40, 0.35])\n",
    "    \n",
    "    # Calculate the mean and standard deviation of the points\n",
    "    mean_point = np.mean(points)\n",
    "    std_dev = np.std(points)\n",
    "    # Create a figure and axis\n",
    "    plt.figure(figsize=(8, 4))\n",
    "    \n",
    "    # Plot the scatter points horizontally\n",
    "    plt.scatter(points, np.zeros_like(points), color='red', label='Scatter points', zorder=5)\n",
    "    \n",
    "    # Overlay the density distribution (Kernel Density Estimate)\n",
    "    sns.kdeplot(points, bw_adjust=0.8, fill=True, color='blue', label='Density Distribution', zorder=1)\n",
    "    plt.hlines(0, xmin=0, xmax=1, colors='black', linestyles='-', label='Horizontal bar', zorder=0)\n",
    "    plt.vlines(mean_point, ymin=0, ymax=3.5, colors='black', linestyles='-', label='Horizontal bar', zorder=0)\n",
    "    \n",
    "     # Plot the arrow representing the standard deviation (from mean-point Â± SD)\n",
    "    plt.annotate('', xy=(mean_point - std_dev, 3.5), xytext=(mean_point, 3.5),\n",
    "                 arrowprops=dict(facecolor='black', arrowstyle='->'))\n",
    "    \n",
    "    # Annotate the arrow with \"SD\" above it\n",
    "    plt.text(mean_point - std_dev / 2, 3.8, f'SD = {std_dev:.2f}', fontsize=10, color='black', ha='left')\n",
    "    \n",
    "    \n",
    "    # Adjusting the plot to focus only on the 0-1 axis for x\n",
    "    plt.xlim(0, 1)\n",
    "    plt.ylim(-0.5, 8)\n",
    "    \n",
    "    # Hide the y-axis as we're focusing on horizontal scatter\n",
    "    plt.gca().axes.get_yaxis().set_visible(False)\n",
    "    \n",
    "    # Add labels and title\n",
    "    plt.xlabel('Predictions')\n",
    "    plt.title('Deep ensemble predictions distribution')\n",
    "    \n",
    "    # Show legend\n",
    "    #plt.legend(loc='upper left')\n",
    "    \n",
    "    # Show the plot\n",
    "    plt.show()\n",
    "\n",
    "# Run the function\n",
    "scatter_with_density()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "uq.UQ_method_plot([distances[k] for k in good_idx], [distances[j] for j in bad_idx], 'min(prob; 1-prob)', 'Distance to hard labels')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "tta_transform = transforms.Compose([\n",
    "        transforms.ToPILImage(),\n",
    "        transforms.RandAugment(2, 9),\n",
    "        transforms.PILToTensor(),\n",
    "        uq.AddBatchDimension(),\n",
    "        transforms.Normalize(mean=mean, std=std)\n",
    "    ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_without_amb_TTA = axialcuts_dataset_eval.data[axialcuts_dataset_eval.data['Shape']!='Ambiguous']\n",
    "data_without_amb_TTA = AxialCutsDataset(data_shape=data_without_amb_TTA, downsample=False)\n",
    "eval_data_TTA = DataLoader(data_without_amb_TTA, num_workers=12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "ll_results = []\n",
    "mean_pred = []\n",
    "true_labels = []\n",
    "stds_tta = []\n",
    "models = [model.to(device) for model in models_list]\n",
    "# Inference\n",
    "with torch.no_grad():\n",
    "    for batch in eval_data_TTA:\n",
    "        tta_pred, std_pred = uq.TTA(tta_transform, models[0], batch['image'], device, nb_augmentations=50)\n",
    "        stds_tta.append(std_pred)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tta_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stds_tta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "uq.UQ_method_plot([stds_tta[k] for k in good_idx], [stds_tta[j] for j in bad_idx], 'Stds', 'Test Time Augmentation (n=50)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_values = uq.standardize_and_max(np.column_stack((stds_tta, stds, distances)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = np.column_stack((stds_tta, stds, distances))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined = a.flatten()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute global mean and standard deviation\n",
    "global_mean = np.mean(combined)\n",
    "global_std_dev = np.std(combined)\n",
    "\n",
    "# Apply z-score standardization to each distribution (column)\n",
    "standardized_distributions = (a - global_mean) / global_std_dev"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "standardized_distributions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find the maximum standardized value for each instance (row)\n",
    "np.mean(standardized_distributions, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "uq.UQ_method_plot([max_values[k] for k in good_idx], [max_values[j] for j in bad_idx], 'Combined metric', 'Combined UQ methods')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fpr_distance_to_gstd, tpr_distance_to_gstd, auc_distance_to_gstd = uq.roc_curve_UQ_method_computation([distances[k] for k in good_idx], [distances[j] for j in bad_idx])\n",
    "fpr_std, tpr_std, auc_std = uq.roc_curve_UQ_method_computation([stds[k] for k in good_idx], [stds[j] for j in bad_idx])\n",
    "fpr_std_tta, tpr_std_tta, auc_std_tta = uq.roc_curve_UQ_method_computation([stds_tta[k] for k in good_idx], [stds_tta[j] for j in bad_idx])\n",
    "fpr_combined, tpr_combined, auc_combined = uq.roc_curve_UQ_method_computation([max_values[k] for k in good_idx], [max_values[j] for j in bad_idx])\n",
    "\n",
    "uq.roc_curve_UQ_methods_plot(['Distance to hard labels', 'Standard deviation models ensembling', 'Standard deviation TTA', 'Method combination'], [fpr_distance_to_gstd, fpr_std, fpr_std_tta, fpr_combined], [tpr_distance_to_gstd, tpr_std, tpr_std_tta, tpr_combined], [auc_distance_to_gstd, auc_std, auc_std_tta, auc_combined])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_to_csv = r'/mnt/data/psteinmetz/neotex/CSV/BOUNDING_BOX.csv'\n",
    "path_to_csv_ville = r'/mnt/data/psteinmetz/neotex/CSV/Bounding_box_ville.csv'\n",
    "path_to_csv_ozgun = r'/mnt/data/psteinmetz/neotex/CSV/ThseOzgun-Kappa_OM_CM_DATA_2023-08-12_1625_relFF.csv'\n",
    "\n",
    "data_train = pd.concat((dt.fread(path_to_csv).to_pandas(), dt.fread(path_to_csv_ville).to_pandas()), ignore_index=True).dropna(axis=1)\n",
    "shape_target_train = pd.concat((data_train[['Record_ID']], data_train[['Shape.1']]), axis=1)\n",
    "shape_modified = shape_target_train.replace('Oval', 'Round')\n",
    "\n",
    "data_ozgun = dt.fread(path_to_csv_ozgun).to_pandas()\n",
    "data_ozgun = data_ozgun[['record_id', 'binaire_shape_exp']]#, 'binaire_shape_junior']]\n",
    "\n",
    "data_ozgun = data_ozgun.replace(True, 'Irregular')\n",
    "data_ozgun = data_ozgun.replace(False, 'Round')\n",
    "\n",
    "# Combine col1 and col2 into a new column 'combined' where values are equal, otherwise NaN\n",
    "#data_ozgun['combined'] = np.where(data_ozgun['binaire_shape_exp'] == data_ozgun['binaire_shape_junior'], data_ozgun['binaire_shape_exp'], np.nan)\n",
    "\n",
    "# Drop rows where 'combined' is NaN, but keep 'ID' and 'combined' columns\n",
    "#data_ozgun = data_ozgun[['record_id', 'combined']].dropna()\n",
    "shape_modified_ozgun = data_ozgun.rename(columns={\"record_id\": \"Record_ID\", \"binaire_shape_exp\": \"Shape.1\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "images_path = '/mnt/data/psteinmetz/neotex/data_CNN/images_15062024/'\n",
    "data = pd.concat(\n",
    "    (\n",
    "        pd.DataFrame(glob.glob(f'{images_path}*/*png')),\n",
    "        pd.DataFrame(\n",
    "            [x.split('/')[-1][:-4] for x in glob.glob(f'{images_path}*/*png')]\n",
    "        ),\n",
    "        pd.DataFrame(\n",
    "            [k.split('/')[-2] for k in glob.glob(f'{images_path}*/*png')]\n",
    "        ),\n",
    "    ),\n",
    "    axis=1,\n",
    ")\n",
    "data.columns = ['Path', 'ID', 'Shape']\n",
    "data.set_index('ID', inplace=True)\n",
    "axialcuts_dataset = AxialCutsDataset(data_shape=data, downsample=True)\n",
    "\n",
    "path_to_csv = r'/mnt/data/psteinmetz/neotex/CSV/BOUNDING_BOX.csv'\n",
    "data_train = dt.fread(path_to_csv).to_pandas()\n",
    "shape_target_train = pd.concat((data_train[['Record_ID']], data_train[['Shape.1']]), axis=1)\n",
    "shape_modified_tr = shape_target_train.replace('Oval', 'Round')\n",
    "shape_modified_tr.set_index('Record_ID', inplace=True)\n",
    "list_cases, index = np.unique((pd.DataFrame(axialcuts_dataset.data.index)['ID'].apply(lambda x: x.split('_')[0])), return_index=True)\n",
    "X = list_cases\n",
    "y = np.array(shape_modified_tr['Shape.1'])\n",
    "skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=123)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "images_path_eval_tr = '/mnt/data/psteinmetz/neotex/data_CNN/model_evaluation/evaluation_training_db'\n",
    "data_eval_tr = pd.concat(\n",
    "    (\n",
    "        pd.DataFrame(glob.glob(f'{images_path_eval_tr}/*png')),\n",
    "        pd.DataFrame(\n",
    "            [x.split('/')[-1][:-4] for x in glob.glob(f'{images_path_eval_tr}/*png')]\n",
    "        ),\n",
    "        pd.DataFrame(\n",
    "            np.zeros(len(glob.glob(f'{images_path_eval_tr}/*png')))\n",
    "        ),\n",
    "    ),\n",
    "    axis=1,\n",
    ")\n",
    "data_eval_tr.columns = ['Path', 'ID', 'Shape']\n",
    "data_eval_tr.set_index('ID', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_cv = []\n",
    "for i, (train_index, test_index) in enumerate(skf.split(X, y)):\n",
    "    ids_tr = pd.DataFrame(data_eval_tr.index)['ID'].apply(lambda x: x.split('_')[0]).astype('int')\n",
    "    index_positions = ids_tr[ids_tr[:].isin(np.sort(ids_tr[:].unique())[test_index])].index\n",
    "    axialcuts_dataset_eval_tr = AxialCutsDataset(data_shape=data_eval_tr.iloc[index_positions], downsample=False, transform=data_transforms)\n",
    "    eval_all_data_tr = DataLoader(axialcuts_dataset_eval_tr, num_workers=12)\n",
    "    \n",
    "    # Store results\n",
    "    all_results = []\n",
    "    mean_pred = []\n",
    "    true_labels = []\n",
    "    names = []\n",
    "    model = [models_list[i].to(device)]\n",
    "    # Inference\n",
    "    with torch.no_grad():\n",
    "        for batch in eval_all_data_tr:\n",
    "            images = batch['image']\n",
    "            labels = batch['shape']\n",
    "            img_names = batch['name']\n",
    "            pred_probs = predict(model, images)\n",
    "            \n",
    "            # Collect the results\n",
    "            mean_pred.append(np.mean(pred_probs))\n",
    "            true_labels.append(labels.item())\n",
    "            all_results.append({\n",
    "                'img_name': img_names[0],\n",
    "                'true_label': labels.item(),\n",
    "                'predicted_probabilities': pred_probs,\n",
    "                'predicted_class': int(np.mean(pred_probs) > 0.5),\n",
    "                'std': np.std(pred_probs),\n",
    "                'mean': np.mean(pred_probs)\n",
    "            })\n",
    "    results_cv.append(all_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_tumor_predictions_with_percentages(results_cv[4], shape_modified)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cm, bal_acc, sensitivity, specificity = classify_tumors_by_contingent_blocks_with_sorting(results_cv[0], shape_modified)\n",
    "\n",
    "print(\"Confusion Matrix:\\n\", cm)\n",
    "print(\"Balanced Accuracy:\", bal_acc)\n",
    "print(\"Sensitivity (Irregular):\", sensitivity)\n",
    "print(\"Specificity (Round):\", specificity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Store results\n",
    "all_results_tr = []\n",
    "mean_pred = []\n",
    "true_labels = []\n",
    "names = []\n",
    "models = [model.to(device) for model in models_list]\n",
    "# Inference\n",
    "with torch.no_grad():\n",
    "    for batch in eval_all_data:\n",
    "        images = batch['image']\n",
    "        labels = batch['shape']\n",
    "        img_names = batch['name']\n",
    "        pred_probs = predict(models, images)\n",
    "        \n",
    "        # Collect the results\n",
    "        mean_pred.append(np.mean(pred_probs))\n",
    "        true_labels.append(labels.item())\n",
    "        all_results_tr.append({\n",
    "            'img_name': img_names[0],\n",
    "            'true_label': labels.item(),\n",
    "            'predicted_probabilities': pred_probs,\n",
    "            'predicted_class': int(np.mean(pred_probs) > 0.5),\n",
    "            'std': np.std(pred_probs),\n",
    "            'mean': np.mean(pred_probs)\n",
    "        })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_tumor_predictions_with_percentages(all_results_tr, shape_modified)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cm_tr, bal_acc_tr, sensitivity_tr, specificity_tr = classify_tumors_by_contingent_blocks_with_sorting(all_results_tr, shape_modified)\n",
    "\n",
    "print(\"Confusion Matrix:\\n\", cm_tr)\n",
    "print(\"Balanced Accuracy:\", bal_acc_tr)\n",
    "print(\"Sensitivity (Irregular):\", sensitivity_tr)\n",
    "print(\"Specificity (Round):\", specificity_tr)\n",
    "display_confusion_matrix(cm_tr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "images_path_eval_ozgun = '/mnt/data/psteinmetz/neotex/data_CNN/model_evaluation/evaluation_Ozgun/'\n",
    "data_eval_ozgun = pd.concat(\n",
    "    (\n",
    "        pd.DataFrame(glob.glob(f'{images_path_eval_ozgun}/*png')),\n",
    "        pd.DataFrame(\n",
    "            [x.split('/')[-1][:-4] for x in glob.glob(f'{images_path_eval_ozgun}/*png')]\n",
    "        ),\n",
    "        pd.DataFrame(\n",
    "            np.zeros(len(glob.glob(f'{images_path_eval_ozgun}/*png')))\n",
    "        ),\n",
    "    ),\n",
    "    axis=1,\n",
    ")\n",
    "data_eval_ozgun.columns = ['Path', 'ID', 'Shape']\n",
    "data_eval_ozgun.set_index('ID', inplace=True)\n",
    "\n",
    "\n",
    "axialcuts_dataset_eval_ozgun = AxialCutsDataset(data_shape=data_eval_ozgun, downsample=False, transform=data_transforms)\n",
    "eval_all_data_ozgun = DataLoader(axialcuts_dataset_eval_ozgun, num_workers=12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Store results\n",
    "all_results_oz = []\n",
    "mean_pred = []\n",
    "true_labels = []\n",
    "names = []\n",
    "models = [model.to(device) for model in models_list]\n",
    "# Inference\n",
    "with torch.no_grad():\n",
    "    for batch in eval_all_data_ozgun:\n",
    "        images = batch['image']\n",
    "        labels = batch['shape']\n",
    "        img_names = batch['name']\n",
    "        pred_probs = predict(models, images)\n",
    "        \n",
    "        # Collect the results\n",
    "        mean_pred.append(np.mean(pred_probs))\n",
    "        true_labels.append(labels.item())\n",
    "        all_results_oz.append({\n",
    "            'img_name': img_names[0],\n",
    "            'true_label': labels.item(),\n",
    "            'predicted_probabilities': pred_probs,\n",
    "            'predicted_class': int(np.mean(pred_probs) > 0.5),\n",
    "            'std': np.std(pred_probs),\n",
    "            'mean': np.mean(pred_probs)\n",
    "        })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_tumor_predictions_with_percentages(all_results_oz, shape_modified_ozgun)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cm_oz, bal_acc_oz, sensitivity_oz, specificity_oz = classify_tumors_by_contingent_blocks_with_sorting(all_results_oz, shape_modified_ozgun)\n",
    "\n",
    "print(\"Confusion Matrix:\\n\", cm_oz)\n",
    "print(\"Balanced Accuracy:\", bal_acc_oz)\n",
    "print(\"Sensitivity (Irregular):\", sensitivity_oz)\n",
    "print(\"Specificity (Round):\", specificity_oz)\n",
    "display_confusion_matrix(cm_oz)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cm, bal_acc, sensitivity, specificity = classify_tumors_by_contingent_blocks_with_sorting(all_results_tr + all_results_oz, pd.concat((shape_modified, shape_modified_ozgun)))\n",
    "\n",
    "print(\"Confusion Matrix:\\n\", cm)\n",
    "print(\"Balanced Accuracy:\", bal_acc)\n",
    "print(\"Sensitivity (Irregular):\", sensitivity)\n",
    "print(\"Specificity (Round):\", specificity)\n",
    "display_confusion_matrix(cm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_results_tr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
